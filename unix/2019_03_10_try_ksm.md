# KSM初体验
## 简介
在Linux-Kernel的2.6.32中，新增了KSM机制来允许通过合并内存页面来降低内存实际使用量。

最初设计上，KSM是用在虚拟机（VM）上的，因为VM都需要加载一些相同的数据，因此如果想办法将这些内存合并到同一块物理内存上，然后修改页表让所有使用进程使用合并之后内存，就可以释放多余的内存，达到节约实际使用内存的目的。

由于原理的可拓展性，实际上内核完成的机制不止可以用于VM之间的内存优化，也可以用于普通的进程之间的内存优化。

## KSM使用示例
[测试源码](https://github.com/WeiKun/KsmCase.git)

首先判断内核是否开启了KSM配置，去检查下config，如果里面的CONFIG_KSM=y那就是已经配置了
```shell
$ cat /boot/config-$(uname -r)|grep KSM
CONFIG_KSM=y
```
如果没有配置则需要重新编译内核。

然后去查看已经启用ksm服务没，/sys/kernel/mm/ksm/run文件内容如果是1，那就已经启用了。
```shell
$ cat /sys/kernel/mm/ksm/run
1
```
如果不为1，那使用"echo "1">/sys/kernel/mm/ksm/run"即可（需要root用户，sudo一般不行，除非先改权限）

再将useksm.c编译成可执行文件并重复运行即可
```shell
$ gcc useksm.c 
$ ./a.out &
[4] 31298
$ ./a.out &
[5] 31299
$ ./a.out &
[6] 31300
$ ./a.out &
[7] 31301
```
之后马上打开top，可以看到ksmd进程会开始占用CPU，used的内存也会逐渐降低并且free出内存。
测试代码中的25K页的数据是完全相同的，因此每一个进程的这些页都会使用同一页实际内存。

```c
#define SIZE 4096 * 1024 * 25

int main()
{
    void *mem = NULL;
    posix_memalign(&mem, 4096, SIZE);
    memset((char *)mem, 0xCC, SIZE);
    int ret = madvise(mem, SIZE, MADV_MERGEABLE);
    sleep(1000);
    return 0 ;
}
```

## KSM机制探索

### merge过程
KSM将页分为两部分存在两棵树里，分别称为stable tree和unstable tree，前者用于组织处于merge状态的页面，后者用于组织尚未出于merge状态的页面。

每次新扫描一个page的时候，会优先判定页面是否与stable tree某页内容一致，如果一致就将其映射关系append到该页上(关键代码如下所示)

```c
/* We first start with searching the page inside the stable tree */
kpage = stable_tree_search(page);
if (kpage == page && rmap_item->head == stable_node) {
	put_page(kpage);
	return;
}

remove_rmap_item_from_tree(rmap_item);

if (kpage) {
	err = try_to_merge_with_ksm_page(rmap_item, page, kpage);
	if (!err) {
		/*
		 * The page was successfully merged:
		 * add its rmap_item to the stable tree.
		 */
		lock_page(kpage);
		stable_tree_append(rmap_item, page_stable_node(kpage),
				   max_page_sharing_bypass);
		unlock_page(kpage);
	}
	put_page(kpage);
	return;
}
```

如果没有在stable tree中查找到匹配的页面，就去unstable tree中查找匹配。如果匹配成功，就将本次的page跟匹配到的页面合并并移动到stable tree。否则就将本次的page作为新节点加入到unstable tree。(关键代码如下所示)
```c
checksum = calc_checksum(page);
if (rmap_item->oldchecksum != checksum) {
	rmap_item->oldchecksum = checksum;
	return;
}

tree_rmap_item =
	unstable_tree_search_insert(rmap_item, page, &tree_page);
if (tree_rmap_item) {
	bool split;

	kpage = try_to_merge_two_pages(rmap_item, page,
					tree_rmap_item, tree_page);
	split = PageTransCompound(page)
		&& compound_head(page) == compound_head(tree_page);
	put_page(tree_page);
	if (kpage) {
		lock_page(kpage);
		stable_node = stable_tree_insert(kpage);
		if (stable_node) {
			stable_tree_append(tree_rmap_item, stable_node,
					   false);
			stable_tree_append(rmap_item, stable_node,
					   false);
		}
		unlock_page(kpage);

		if (!stable_node) {
			break_cow(tree_rmap_item);
			break_cow(rmap_item);
		}
	} else if (split) {
		if (!trylock_page(page))
			return;
		split_huge_page(page);
		unlock_page(page);
	}
}

```
两次匹配时均使用memcmp进行对比，由于大部分的页面进行memcmp时会很快返回，因此这样并不比使用md5值之类的低效。

对于stable tree内的页面，会被设置为写保护，一旦某个进程对页面的某个副本进行写操作，类似于fork进程之后的内存行为，会触发COW(copy-on-write)，将页面复制一份之后修改映射关系，然后在这之上进行写操作。

对于unstable tree内的页面，由于缺乏写保护，因此其在树上的排列关系是不稳定的，因此每次scan都会进行丢弃并重建。

### 跟共享内存比较
KSM跟共享内存的实际原理比较类似，都是利用虚拟地址映射机制在不同进程共用一份物理内存。但是共享内存是显示指明其管理方式的，而KSM只是标记内存允许合并，具体是否合并跟怎么合并由后台的KSMD进程进行处理。

共享内存的好处是由使用者指定，因此不需要后台监控进程，执行效率会相对高一点

而KSM正由于不需要使用者显示指定，因此可以更自由地使用(例如如果100M数据里恰好有几个指针数据由于指向的是本进程的一些全局变量/函数，因此各个进程会不一样，这时候想使用共享内存就会很痛苦，而KSM机制却可以良好地运行)。

### KSM适用范围
虽然没真的用，但是从KSM的机制上，不难分析出KSM适用的优化数据，应该具有稳定之后unstable tree中的节点数尽可能少跟stable tree中页面触发COW频率尽量低的特点，否则就会导致KSMD扫描效率过低跟频繁copy页的问题。

这样，数据就应该是各进程中尽量一致性高的，并且不会出现改动的数据。比较典型的就是一些预生成的数据跟资源。